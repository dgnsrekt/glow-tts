# Performance Standards

> Guidelines for optimizing memory, CPU, and audio processing performance
> Focus on profiling, benchmarking, and optimization strategies

## Performance Goals

### Target Metrics
- **TTS Initialization**: <3 seconds
- **Sentence Processing**: <200ms per sentence
- **UI Response Time**: <50ms for user input
- **Memory Usage**: <75MB with TTS active
- **Audio Latency**: <100ms from request to playback
- **Cache Hit Rate**: >80% for repeated content

## Memory Management

### Efficient Allocations
```go
// Preallocate slices when size is known
func ProcessSentences(count int) []Sentence {
    sentences := make([]Sentence, 0, count)  // Preallocate capacity
    // Process...
    return sentences
}

// Reuse buffers with sync.Pool
var bufferPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 4096)
    },
}

func ProcessAudio(data []byte) []byte {
    buf := bufferPool.Get().([]byte)
    defer bufferPool.Put(buf)
    
    // Use buffer...
    return buf[:n]
}
```

### Memory-Efficient Structures
```go
// Bad: Wasteful struct padding
type BadStruct struct {
    flag bool    // 1 byte + 7 padding
    id   int64   // 8 bytes
    name string  // 16 bytes
    age  int32   // 4 bytes + 4 padding
}  // Total: 40 bytes

// Good: Optimized field ordering
type GoodStruct struct {
    id   int64   // 8 bytes
    name string  // 16 bytes  
    age  int32   // 4 bytes
    flag bool    // 1 byte + 3 padding
}  // Total: 32 bytes

// Use struct tags for cache alignment
type CacheLineAligned struct {
    _ [64]byte // Cache line padding
    counter uint64
    _ [56]byte // Pad to next cache line
}
```

### String Optimization
```go
// Avoid string concatenation in loops
func BadConcat(words []string) string {
    result := ""
    for _, word := range words {
        result += word + " "  // Creates new string each time
    }
    return result
}

func GoodConcat(words []string) string {
    var sb strings.Builder
    sb.Grow(len(words) * 10)  // Preallocate estimated size
    for _, word := range words {
        sb.WriteString(word)
        sb.WriteByte(' ')
    }
    return sb.String()
}

// String to byte conversion without allocation
func StringToBytes(s string) []byte {
    return unsafe.Slice(unsafe.StringData(s), len(s))
}

// Byte to string conversion without allocation
func BytesToString(b []byte) string {
    return unsafe.String(&b[0], len(b))
}
```

## Audio Buffer Management

### Ring Buffer Implementation
```go
type RingBuffer struct {
    data     []byte
    size     int
    head     int
    tail     int
    mu       sync.Mutex
    notFull  *sync.Cond
    notEmpty *sync.Cond
}

func NewRingBuffer(size int) *RingBuffer {
    rb := &RingBuffer{
        data: make([]byte, size),
        size: size,
    }
    rb.notFull = sync.NewCond(&rb.mu)
    rb.notEmpty = sync.NewCond(&rb.mu)
    return rb
}

func (rb *RingBuffer) Write(data []byte) (int, error) {
    rb.mu.Lock()
    defer rb.mu.Unlock()
    
    written := 0
    for len(data) > 0 {
        // Wait if buffer is full
        for rb.isFull() {
            rb.notFull.Wait()
        }
        
        // Write what we can
        n := rb.writeAvailable()
        if n > len(data) {
            n = len(data)
        }
        
        copy(rb.data[rb.tail:], data[:n])
        rb.tail = (rb.tail + n) % rb.size
        data = data[n:]
        written += n
        
        rb.notEmpty.Signal()
    }
    
    return written, nil
}
```

### Memory-Mapped Audio Cache
```go
type MMapCache struct {
    file *os.File
    data []byte
    size int64
}

func NewMMapCache(path string, size int64) (*MMapCache, error) {
    file, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, 0600)
    if err != nil {
        return nil, err
    }
    
    if err := file.Truncate(size); err != nil {
        file.Close()
        return nil, err
    }
    
    data, err := mmap.Map(file, mmap.RDWR, 0)
    if err != nil {
        file.Close()
        return nil, err
    }
    
    return &MMapCache{
        file: file,
        data: data,
        size: size,
    }, nil
}

func (c *MMapCache) Close() error {
    if err := mmap.Unmap(c.data); err != nil {
        return err
    }
    return c.file.Close()
}
```

## CPU Optimization

### Parallel Processing
```go
// Process sentences in parallel
func ParallelSynthesize(engine TTSEngine, sentences []string) [][]byte {
    results := make([][]byte, len(sentences))
    
    // Use worker pool to limit concurrency
    workers := runtime.NumCPU()
    sem := make(chan struct{}, workers)
    
    var wg sync.WaitGroup
    for i, sentence := range sentences {
        wg.Add(1)
        sem <- struct{}{} // Acquire semaphore
        
        go func(idx int, text string) {
            defer wg.Done()
            defer func() { <-sem }() // Release semaphore
            
            audio, err := engine.Synthesize(context.Background(), text)
            if err == nil {
                results[idx] = audio
            }
        }(i, sentence)
    }
    
    wg.Wait()
    return results
}
```

### SIMD Optimization
```go
// Use SIMD for audio processing where available
//go:build amd64 || arm64

package audio

import (
    "golang.org/x/sys/cpu"
)

func init() {
    if cpu.X86.HasAVX2 {
        processAudio = processAudioAVX2
    } else if cpu.ARM64.HasNEON {
        processAudio = processAudioNEON
    } else {
        processAudio = processAudioGeneric
    }
}

// Assembly implementations
func processAudioAVX2(data []byte) []byte
func processAudioNEON(data []byte) []byte
func processAudioGeneric(data []byte) []byte {
    // Fallback implementation
    for i := range data {
        data[i] = data[i] / 2 // Simple volume reduction
    }
    return data
}
```

## Caching Strategies

### LRU Cache Implementation
```go
type LRUCache struct {
    capacity int
    size     int64
    items    map[string]*list.Element
    eviction *list.List
    mu       sync.RWMutex
}

type cacheEntry struct {
    key   string
    value []byte
    size  int64
}

func NewLRUCache(capacity int) *LRUCache {
    return &LRUCache{
        capacity: capacity,
        items:    make(map[string]*list.Element),
        eviction: list.New(),
    }
}

func (c *LRUCache) Get(key string) ([]byte, bool) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    if elem, ok := c.items[key]; ok {
        c.eviction.MoveToFront(elem)
        entry := elem.Value.(*cacheEntry)
        return entry.value, true
    }
    return nil, false
}

func (c *LRUCache) Set(key string, value []byte) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    // Check if already exists
    if elem, ok := c.items[key]; ok {
        c.eviction.MoveToFront(elem)
        entry := elem.Value.(*cacheEntry)
        c.size += int64(len(value)) - entry.size
        entry.value = value
        entry.size = int64(len(value))
        return
    }
    
    // Add new entry
    entry := &cacheEntry{
        key:   key,
        value: value,
        size:  int64(len(value)),
    }
    
    elem := c.eviction.PushFront(entry)
    c.items[key] = elem
    c.size += entry.size
    
    // Evict if necessary
    for c.eviction.Len() > c.capacity {
        c.evictOldest()
    }
}

func (c *LRUCache) evictOldest() {
    elem := c.eviction.Back()
    if elem != nil {
        c.eviction.Remove(elem)
        entry := elem.Value.(*cacheEntry)
        delete(c.items, entry.key)
        c.size -= entry.size
    }
}
```

### Bloom Filter for Cache
```go
// Use bloom filter to avoid unnecessary cache lookups
type CacheWithBloom struct {
    cache  *LRUCache
    bloom  *BloomFilter
    mu     sync.RWMutex
}

func (c *CacheWithBloom) Get(key string) ([]byte, bool) {
    // Quick check with bloom filter
    if !c.bloom.MightContain(key) {
        return nil, false
    }
    
    // Actual cache lookup
    return c.cache.Get(key)
}

func (c *CacheWithBloom) Set(key string, value []byte) {
    c.bloom.Add(key)
    c.cache.Set(key, value)
}
```

## Profiling Tools

### CPU Profiling
```go
import (
    "runtime/pprof"
    "os"
)

func StartCPUProfile(file string) func() {
    f, err := os.Create(file)
    if err != nil {
        log.Fatal(err)
    }
    
    pprof.StartCPUProfile(f)
    
    return func() {
        pprof.StopCPUProfile()
        f.Close()
    }
}

// Usage in main or test
func main() {
    defer StartCPUProfile("cpu.prof")()
    // Run application
}

// Analyze: go tool pprof cpu.prof
```

### Memory Profiling
```go
func WriteMemProfile(file string) {
    f, err := os.Create(file)
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()
    
    runtime.GC() // Force GC for accurate profile
    if err := pprof.WriteHeapProfile(f); err != nil {
        log.Fatal(err)
    }
}

// Memory stats monitoring
func MonitorMemory() {
    var m runtime.MemStats
    
    ticker := time.NewTicker(10 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        runtime.ReadMemStats(&m)
        log.Printf("Alloc: %v MB, Sys: %v MB, NumGC: %v",
            m.Alloc/1024/1024,
            m.Sys/1024/1024,
            m.NumGC,
        )
    }
}
```

### Trace Analysis
```go
import "runtime/trace"

func StartTrace(file string) func() {
    f, err := os.Create(file)
    if err != nil {
        log.Fatal(err)
    }
    
    trace.Start(f)
    
    return func() {
        trace.Stop()
        f.Close()
    }
}

// Analyze: go tool trace trace.out
```

## Benchmarking

### Micro Benchmarks
```go
func BenchmarkSentenceParsing(b *testing.B) {
    text := strings.Repeat("This is a sentence. ", 100)
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        _ = ParseSentences(text)
    }
}

// Comparative benchmarks
func BenchmarkEngines(b *testing.B) {
    text := "Benchmark test sentence."
    
    b.Run("Piper", func(b *testing.B) {
        engine := NewPiperEngine()
        defer engine.Close()
        
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            _, _ = engine.Synthesize(context.Background(), text)
        }
    })
    
    b.Run("GoogleTTS", func(b *testing.B) {
        engine := NewGoogleTTSEngine()
        defer engine.Close()
        
        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            _, _ = engine.Synthesize(context.Background(), text)
        }
    })
}
```

### Load Testing
```go
func BenchmarkConcurrentLoad(b *testing.B) {
    engine := NewEngine()
    defer engine.Close()
    
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            _, _ = engine.Synthesize(context.Background(), "Test")
        }
    })
}

// Measure throughput
func BenchmarkThroughput(b *testing.B) {
    engine := NewEngine()
    defer engine.Close()
    
    data := make([]string, 1000)
    for i := range data {
        data[i] = fmt.Sprintf("Sentence %d", i)
    }
    
    b.ResetTimer()
    start := time.Now()
    
    for i := 0; i < b.N; i++ {
        for _, text := range data {
            _, _ = engine.Synthesize(context.Background(), text)
        }
    }
    
    elapsed := time.Since(start)
    throughput := float64(b.N*len(data)) / elapsed.Seconds()
    b.ReportMetric(throughput, "sentences/sec")
}
```

## Optimization Techniques

### Lazy Evaluation
```go
type LazyAudio struct {
    text     string
    engine   TTSEngine
    audio    []byte
    once     sync.Once
    err      error
}

func (l *LazyAudio) Get() ([]byte, error) {
    l.once.Do(func() {
        l.audio, l.err = l.engine.Synthesize(context.Background(), l.text)
    })
    return l.audio, l.err
}
```

### Batching Operations
```go
type BatchProcessor struct {
    items    []Item
    mu       sync.Mutex
    cond     *sync.Cond
    maxBatch int
    maxWait  time.Duration
}

func (b *BatchProcessor) Add(item Item) {
    b.mu.Lock()
    b.items = append(b.items, item)
    
    if len(b.items) >= b.maxBatch {
        b.flush()
    }
    b.mu.Unlock()
}

func (b *BatchProcessor) flush() {
    if len(b.items) == 0 {
        return
    }
    
    batch := b.items
    b.items = nil
    
    go b.processBatch(batch)
}
```

## Performance Monitoring

### Metrics Collection
```go
type Metrics struct {
    synthesisTime   *prometheus.HistogramVec
    cacheHitRate    *prometheus.CounterVec
    memoryUsage     *prometheus.GaugeVec
    activeGoroutines prometheus.Gauge
}

func NewMetrics() *Metrics {
    m := &Metrics{
        synthesisTime: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "tts_synthesis_duration_seconds",
                Help:    "Time taken to synthesize text",
                Buckets: prometheus.DefBuckets,
            },
            []string{"engine"},
        ),
        // ... other metrics
    }
    
    prometheus.MustRegister(m.synthesisTime)
    // ... register others
    
    return m
}

func (m *Metrics) RecordSynthesis(engine string, duration time.Duration) {
    m.synthesisTime.WithLabelValues(engine).Observe(duration.Seconds())
}
```

### Performance Regression Tests
```go
func TestPerformanceRegression(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping performance test")
    }
    
    baseline := 100 * time.Millisecond
    tolerance := 1.2 // 20% tolerance
    
    start := time.Now()
    // Perform operation
    ProcessSentence("Test sentence")
    duration := time.Since(start)
    
    if duration > time.Duration(float64(baseline)*tolerance) {
        t.Errorf("Performance regression: took %v, baseline %v", 
            duration, baseline)
    }
}
```

## Optimization Checklist

### Before Optimizing
- [ ] Profile to identify bottlenecks
- [ ] Benchmark current performance
- [ ] Set performance goals
- [ ] Consider algorithmic improvements first

### During Optimization
- [ ] Focus on hot paths (80/20 rule)
- [ ] Measure impact of each change
- [ ] Document optimization rationale
- [ ] Consider maintenance cost

### After Optimization
- [ ] Verify correctness with tests
- [ ] Update benchmarks
- [ ] Document performance characteristics
- [ ] Set up monitoring

---

*Optimize for the common case, but handle edge cases correctly.*